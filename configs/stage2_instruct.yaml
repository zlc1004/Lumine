# Lumine Stage 2: Blackwell-Optimized Instruction-Following
# Base: Lumine-Agent-Pretraining-VL-7B
# 2x B200 (384GB Total VRAM) | Goal: Language-to-Action Grounding

model:
  model_path: output/Lumine-Agent-Pretraining-VL-7B
  attn_implementation: flash_attention_2

data:
  train_path: configs/data_instruct.yaml
  chat_template: qwen2vl
  # Higher context for instruction + 8-frame history + reasoning tokens
  max_seq_len: 16384 
  train_size: 1000000
  datasets_type: mapping
  mm_configs:
    image_max_pixels: 921600 # Keeping 720p for precise grounding
    video_max_pixels: 921600 
    max_frames: 8
    fps: 2.0

train:
  output_dir: output/Lumine-Agent-Instruct-VL-7B
  data_parallel_mode: fsdp2
  wandb_project: lumine
  wandb_name: lumine_stage2_b200_720p
  rmpad: true # Significant speedup for varying instruction lengths
  rmpad_with_pos_ids: true
  ulysses_parallel_size: 1
  freeze_vit: false # Fine-tuning ViT is key for grounding instructions
  lr: 5.0e-6        # Slightly lower LR for Stage 2 to prevent catastrophic forgetting
  init_device: meta
  lr_decay_style: cosine
  num_train_epochs: 3
  micro_batch_size: 2 # B200 comfortably handles MBZ 2 with 16k context
  global_batch_size: 16 # Tighter batch size for instruction tuning
  max_steps: 5000
  save_steps: 500
  save_hf_weights: true
  # PERFORMANCE TUNING
  enable_gradient_checkpointing: false # Disabled for max throughput