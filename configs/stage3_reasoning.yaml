# Lumine Stage 3: Blackwell-Optimized Reasoning (Thinking)
# Base: Lumine-Agent-Instruct-VL-7B
# 2x B200 (384GB Total VRAM) | Goal: Long-form Chain-of-Thought Reasoning

model:
  model_path: output/Lumine-Agent-Instruct-VL-7B
  attn_implementation: flash_attention_2

data:
  train_path: configs/data_reasoning.yaml
  chat_template: qwen2vl
  # Expanded to 32k to handle long CoT reasoning + 8-frame 720p history
  max_seq_len: 32768 
  train_size: 100000
  datasets_type: mapping
  mm_configs:
    image_max_pixels: 921600 # Restored 720p for high-detail reasoning
    video_max_pixels: 921600 
    max_frames: 8
    fps: 2.0

train:
  output_dir: output/Lumine-Agent-Thinking-VL-7B
  data_parallel_mode: fsdp2
  wandb_project: lumine
  wandb_name: lumine_stage3_b200_reasoning
  rmpad: true # Critical for 32k window to maintain speed
  rmpad_with_pos_ids: true
  ulysses_parallel_size: 1
  # Strategy: Unfreeze ViT if detail matters, but Stage 3 often focuses on the LLM
  freeze_vit: false 
  lr: 5.0e-6        # Low LR for fine-tuning reasoning capabilities
  init_device: meta
  lr_decay_style: cosine
  num_train_epochs: 2
  micro_batch_size: 1 # 32k context + MBZ 1 is safer for peak CoT samples
  global_batch_size: 8 
  max_steps: 2000
  save_steps: 500
  save_hf_weights: true
  # PERFORMANCE TUNING
  enable_gradient_checkpointing: false # Still disabled for B200 speed